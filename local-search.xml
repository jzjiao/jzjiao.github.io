<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>微调大语言模型之参数篇</title>
    <link href="/2023/11/16/%E5%BE%AE%E8%B0%83%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B9%8B%E5%8F%82%E6%95%B0%E7%AF%87/"/>
    <url>/2023/11/16/%E5%BE%AE%E8%B0%83%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B9%8B%E5%8F%82%E6%95%B0%E7%AF%87/</url>
    
    <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>在进行大语言模型微调时，需要设置很多参数，比如数据集的名称&#x2F;路径、模型的名称&#x2F;路径、学习率大小和batchsize等参数。那么我们需要将哪些参数设置成外部读取，这些参数的默认值是什么，以及都有什么作用？本篇文章基于<a href="https://github.com/FlagAlpha/Llama2-Chinese/tree/main">Llama2-Chinese</a>中的<strong>train&#x2F;sft&#x2F;finetune_clm_lora.py</strong>进行参数解析，希望可以从入参的角度对训练过程有整体把握。</p><h2 id="dataclass装饰符介绍"><a href="#dataclass装饰符介绍" class="headerlink" title="@dataclass装饰符介绍"></a>@dataclass装饰符介绍</h2><p>在进行具体参数介绍前，首先介绍一下** @dataclass **</p>]]></content>
    
    
    <categories>
      
      <category>LLM</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>SFT</tag>
      
      <tag>LoRA</tag>
      
      <tag>TrainingArguments</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
